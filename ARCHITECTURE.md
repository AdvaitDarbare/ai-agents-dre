```mermaid
graph TD
    Data[ğŸ“„ Data File] --> Monitor
    
    subgraph "ğŸ•µï¸ AGENT 1: MONITOR (Enhanced)"
        Monitor[Monitor Agent]
        subgraph "Core Tools"
            T1[File Metadata]
            T2[Data Loader]
            T3[Schema Validator]
            T4[Stats Analysis]
            T5[Drift Check]
        end
        subgraph "Databricks-Inspired Tools"
            T6[ğŸ†• Seasonal Detector]
            T7[ğŸ†• Table Prioritizer]
            T8[ğŸ†• Quality Metrics]
            T9[ğŸ†• Health Indicator]
        end
        Monitor --- T1 & T2 & T3 & T4 & T5
        Monitor --- T6 & T7 & T8 & T9
    end
    
    Monitor -->|âœ… PASS| Database[ğŸ’¾ Approved Data]
    Monitor -->|âŒ FAIL| Manual[ğŸ›‘ Manual Intervention]
    Monitor -->|âš ï¸ PASS WITH WARNINGS| Diagnoser
    
    subgraph "REMEDIATION PIPELINE"
        Diagnoser[ğŸ” Diagnoser Agent] -->|Auto-Fixable| Healer[ğŸ”§ Healer Agent]
        Diagnoser -->|Not Fixable| Manual
        Healer --> Validator[âœ… Validator Agent]
        Validator -->|Approved| Database
        Validator -->|Rejected| Manual
    end
    
    classDef green fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px;
    classDef red fill:#ffebee,stroke:#c62828,stroke-width:2px;
    classDef blue fill:#e3f2fd,stroke:#1565c0,stroke-width:2px;
    classDef new fill:#fff3e0,stroke:#ef6c00,stroke-width:2px;
    
    class Database,Validator green;
    class Manual red;
    class Monitor,Diagnoser,Healer blue;
    class T6,T7,T8,T9 new;
```

## ğŸ†• Databricks-Inspired Enhancements

Based on [Databricks' Agentic Data Quality Monitoring](https://www.databricks.com/blog/data-quality-monitoring-scale-agentic-ai), we've added:

### 1. Seasonal Detector
**Learned behavior, not static rules.**
- Learns day-of-week patterns (e.g., dips on weekends)
- Learns monthly patterns (e.g., tax season spikes)
- Adapts thresholds based on historical variance
- Distinguishes real anomalies from expected seasonal variation

### 2. Table Prioritizer
**Issues prioritized by downstream impact.**
- Tables ranked by certification level (Gold/Silver/Bronze)
- Downstream lineage tracking
- Query volume monitoring
- Freshness SLA enforcement
- High-impact tables are flagged first

### 3. Quality Metrics Tool
**Comprehensive data profiling.**
- **Freshness**: Is the data up-to-date?
- **Completeness**: How much data is missing?
- **Validity**: Does data conform to expected formats?
- **Uniqueness**: Are there unexpected duplicates?

### 4. Health Indicator
**Consistent health signals across the platform.**
- Unified health score (0-100)
- Clear "safe to use" indication
- Status badges for dashboards
- Schema-level health aggregation

## Monitor Agent: The Decision Tree

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      MONITOR AGENT DECISION TREE                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: Sanity Check (FileMetadataTool)
â”œâ”€ Check file freshness (< 24 hours?)
â”œâ”€ Calculate MD5 hash (duplicate?)
â””â”€ Get file size
   â”‚
   â”œâ”€ âŒ STALE/DUPLICATE â†’ STOP & ARCHIVE
   â””â”€ âœ… FRESH â†’ CONTINUE

STEP 2: Load & Sampling (DataLoaderTool)
â”œâ”€ File > 500MB?
â”‚  â”œâ”€ YES â†’ Load 10% sample
â”‚  â””â”€ NO â†’ Load full file
â””â”€ Parse CSV/Parquet/JSON
   â”‚
   â”œâ”€ âŒ LOAD FAILED â†’ CRITICAL STOP
   â””â”€ âœ… LOADED â†’ CONTINUE

STEP 3: Schema Validation (SchemaValidatorTool)
â”œâ”€ Compare against monitor_config.yaml
â”œâ”€ Check for missing columns
â”œâ”€ Check for extra columns
â”œâ”€ Validate data types
â””â”€ Check constraints (null%, unique, min/max)
   â”‚
   â”œâ”€ âŒ CRITICAL VIOLATION â†’ CRITICAL STOP
   â”œâ”€ âš ï¸  WARNING VIOLATION â†’ WARN & CONTINUE
   â””â”€ âœ… PASS â†’ CONTINUE

STEP 4: Statistical Profiling (StatsAnalysisTool)
â”œâ”€ Calculate: mean, median, std, skewness, kurtosis
â”œâ”€ Adaptive Outlier Detection:
â”‚  â”œâ”€ Skewness < 1.0 â†’ Z-Score method (threshold=3.0)
â”‚  â””â”€ Skewness â‰¥ 1.0 â†’ IQR method (multiplier=1.5)
â””â”€ Identify outlier indices
   â”‚
   â””â”€ âœ… PROFILED â†’ CONTINUE

STEP 5: Drift Detection (DriftCheckTool)
â”œâ”€ Query SQLite for 7-day historical baseline
â”œâ”€ Compare current metrics vs baseline
â”‚  â”œâ”€ Row count deviation
â”‚  â””â”€ Column mean deviation
â””â”€ Check if deviation > 30%
   â”‚
   â”œâ”€ âš ï¸  DRIFT DETECTED â†’ WARN & CONTINUE
   â””â”€ âœ… NO DRIFT â†’ CONTINUE

STEP 6: Final Verdict
â”œâ”€ Aggregate all CRITICAL errors
â”œâ”€ Aggregate all WARNINGS
â”œâ”€ Collect quarantine indices (outliers)
â””â”€ Generate JSON report
   â”‚
   â”œâ”€ âŒ CRITICAL ERRORS â†’ STATUS: FAIL
   â”œâ”€ âš ï¸  WARNINGS â†’ STATUS: PASS_WITH_WARNINGS
   â””â”€ âœ… NO ISSUES â†’ STATUS: PASS
```

## Tool Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          MONITOR AGENT TOOLS                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

tools/monitor/
â”‚
â”œâ”€ file_metadata_tool.py
â”‚  â”œâ”€ Check file age (freshness)
â”‚  â”œâ”€ Calculate MD5 hash (duplication)
â”‚  â”œâ”€ Get file size
â”‚  â””â”€ Decision: CONTINUE or STOP
â”‚
â”œâ”€ data_loader_tool.py
â”‚  â”œâ”€ Detect file type (CSV/Parquet/JSON)
â”‚  â”œâ”€ Smart sampling (if > 500MB)
â”‚  â”œâ”€ Load into pandas DataFrame
â”‚  â””â”€ Decision: CONTINUE or CRITICAL_STOP
â”‚
â”œâ”€ schema_validator_tool.py
â”‚  â”œâ”€ Load YAML config
â”‚  â”œâ”€ Check missing/extra columns
â”‚  â”œâ”€ Validate data types
â”‚  â”œâ”€ Check constraints (null%, unique, min/max)
â”‚  â””â”€ Decision: CONTINUE or CRITICAL_STOP
â”‚
â”œâ”€ stats_analysis_tool.py
â”‚  â”œâ”€ Calculate statistics (mean, median, std, skew, kurt)
â”‚  â”œâ”€ Adaptive outlier detection:
â”‚  â”‚  â”œâ”€ Normal distribution â†’ Z-Score
â”‚  â”‚  â””â”€ Skewed distribution â†’ IQR
â”‚  â””â”€ Return outlier indices
â”‚
â””â”€ drift_check_tool.py
   â”œâ”€ Initialize SQLite database
   â”œâ”€ Query 7-day historical baseline
   â”œâ”€ Compare current vs baseline
   â”œâ”€ Calculate deviation percentage
   â””â”€ Decision: WARN or PASS
```

## Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              DATA FLOW                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT:
  â”œâ”€ File Path: data/transactions.csv
  â”œâ”€ Table Name: transactions
  â””â”€ Config: config/monitor_config.yaml

PROCESSING:
  â”œâ”€ FileMetadataTool â†’ { status, size_mb, hash, decision }
  â”œâ”€ DataLoaderTool â†’ { dataframe, rows_loaded, columns, decision }
  â”œâ”€ SchemaValidatorTool â†’ { violations[], decision }
  â”œâ”€ StatsAnalysisTool â†’ { profiles{}, outlier_indices[] }
  â””â”€ DriftCheckTool â†’ { drift_warnings[], decision }

OUTPUT:
  â””â”€ JSON Report:
     {
       "timestamp": "2026-02-05T...",
       "file": "data/transactions.csv",
       "status": "PASS_WITH_WARNINGS",
       "execution_time": "0.02s",
       "critical_errors": [],
       "warnings": ["Column 'category': null% = 5.5%"],
       "stats_summary": { ... },
       "quarantine_indices": [45, 92, 101, ...],
       "execution_log": [ ... ]
     }

STORAGE:
  â”œâ”€ SQLite: data/metrics_history.db (historical metrics)
  â””â”€ JSON: reports/monitor_report_YYYYMMDD_HHMMSS.json
```

## YAML Configuration Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       DATA CONTRACT (YAML)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

monitor_config.yaml
â”‚
â”œâ”€ defaults:
â”‚  â”œâ”€ freshness_hours: 24
â”‚  â”œâ”€ sampling_threshold_mb: 500
â”‚  â”œâ”€ drift_threshold_pct: 30
â”‚  â””â”€ lookback_days: 7
â”‚
â””â”€ columns:
   â”‚
   â”œâ”€ CRITICAL Columns (Hard Stops):
   â”‚  â”œâ”€ transaction_id (unique, no nulls)
   â”‚  â”œâ”€ amount (min: 0.01, no nulls)
   â”‚  â””â”€ timestamp (no nulls)
   â”‚
   â””â”€ WARNING Columns (Soft Fails):
      â”œâ”€ user_id (max 10% nulls)
      â”œâ”€ user_comment (max 50% nulls)
      â””â”€ category (max 5% nulls)
```

---

**Legend:**
- âœ… = Fully Implemented
- ğŸ“ = Placeholder (Ready for Implementation)
- âŒ = Critical Stop
- âš ï¸  = Warning (Continue)
# ğŸŒ System Architecture (Mermaid Diagram)

You can copy the code below into any Mermaid live editor (like [mermaid.live](https://mermaid.live/)) to generate a high-quality image.

```mermaid
graph TD
    %% Styling
    classDef monitor fill:#e3f2fd,stroke:#1565c0,stroke-width:2px,rx:10;
    classDef core fill:#bbdefb,stroke:#1976d2,stroke-width:1px,rx:5;
    classDef enhanced fill:#ffecb3,stroke:#ffa000,stroke-width:2px,rx:5;
    classDef output fill:#c8e6c9,stroke:#388e3c,stroke-width:2px,rx:5;
    classDef fail fill:#ffcdd2,stroke:#d32f2f,stroke-width:2px,rx:5;
    classDef future fill:#f5f5f5,stroke:#9e9e9e,stroke-width:1px,stroke-dasharray: 5 5,rx:5;

    %% Data Input
    Data([ğŸ“„ Data File\nCSV/JSON/Parquet]) --> MonitorAgent

    %% Main Agent
    subgraph MonitorAgent ["ğŸ•µï¸ MONITOR AGENT (The Gatekeeper)"]
        direction TB
        
        subgraph Valid ["Phase 1: Validation"]
            T1[File Metadata]:::core
            T2[Data Loader]:::core
            T3[Schema Check]:::core
        end
        
        subgraph Intel ["Phase 2: Intelligence"]
            T4[Stats Analysis]:::core
            T5[Drift Check]:::core
            T6[ğŸŒ Seasonal Detector]:::enhanced
        end
        
        subgraph Impact ["Phase 3: Impact Analysis"]
            T7[Quality Metrics]:::enhanced
            T8[Health Indicator]:::enhanced
            T9[Table Prioritizer]:::enhanced
        end
        
        Valid --> Intel --> Impact
    end

    %% Outputs
    Impact --> Decision{VERDICT}
    
    Decision -->|âœ… PASS| Approved[ğŸ’¾ Approved Data\n(To Warehouse)]:::output
    Decision -->|âŒ FAIL| ManAuth[ğŸ›‘ Manual Authorization\nRequired]:::fail
    Decision -->|âš ï¸ WARN| Diagnoser
    
    %% Future Agents
    subgraph Remediation ["ğŸ¤– REMEDIATION PIPELINE (Future)"]
        Diagnoser[ğŸ” Diagnoser Agent]:::future -->|Analysis| Healer[ğŸ”§ Healer Agent]:::future
        Healer -->|Fixes| Validator[âœ… Validator Agent]:::future
        Validator --> Approved
    end
    
    %% Legend links
    class MonitorAgent monitor;
```
